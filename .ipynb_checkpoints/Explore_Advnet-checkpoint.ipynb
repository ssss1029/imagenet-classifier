{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import ImageOps, Image\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_r_wnids = ['n01443537', 'n01484850', 'n01494475', 'n01498041', 'n01514859', 'n01518878', 'n01531178', 'n01534433', 'n01614925', 'n01616318', 'n01630670', 'n01632777', 'n01644373', 'n01677366', 'n01694178', 'n01748264', 'n01770393', 'n01774750', 'n01784675', 'n01806143', 'n01820546', 'n01833805', 'n01843383', 'n01847000', 'n01855672', 'n01860187', 'n01882714', 'n01910747', 'n01944390', 'n01983481', 'n01986214', 'n02007558', 'n02009912', 'n02051845', 'n02056570', 'n02066245', 'n02071294', 'n02077923', 'n02085620', 'n02086240', 'n02088094', 'n02088238', 'n02088364', 'n02088466', 'n02091032', 'n02091134', 'n02092339', 'n02094433', 'n02096585', 'n02097298', 'n02098286', 'n02099601', 'n02099712', 'n02102318', 'n02106030', 'n02106166', 'n02106550', 'n02106662', 'n02108089', 'n02108915', 'n02109525', 'n02110185', 'n02110341', 'n02110958', 'n02112018', 'n02112137', 'n02113023', 'n02113624', 'n02113799', 'n02114367', 'n02117135', 'n02119022', 'n02123045', 'n02128385', 'n02128757', 'n02129165', 'n02129604', 'n02130308', 'n02134084', 'n02138441', 'n02165456', 'n02190166', 'n02206856', 'n02219486', 'n02226429', 'n02233338', 'n02236044', 'n02268443', 'n02279972', 'n02317335', 'n02325366', 'n02346627', 'n02356798', 'n02363005', 'n02364673', 'n02391049', 'n02395406', 'n02398521', 'n02410509', 'n02423022', 'n02437616', 'n02445715', 'n02447366', 'n02480495', 'n02480855', 'n02481823', 'n02483362', 'n02486410', 'n02510455', 'n02526121', 'n02607072', 'n02655020', 'n02672831', 'n02701002', 'n02749479', 'n02769748', 'n02793495', 'n02797295', 'n02802426', 'n02808440', 'n02814860', 'n02823750', 'n02841315', 'n02843684', 'n02883205', 'n02906734', 'n02909870', 'n02939185', 'n02948072', 'n02950826', 'n02951358', 'n02966193', 'n02980441', 'n02992529', 'n03124170', 'n03272010', 'n03345487', 'n03372029', 'n03424325', 'n03452741', 'n03467068', 'n03481172', 'n03494278', 'n03495258', 'n03498962', 'n03594945', 'n03602883', 'n03630383', 'n03649909', 'n03676483', 'n03710193', 'n03773504', 'n03775071', 'n03888257', 'n03930630', 'n03947888', 'n04086273', 'n04118538', 'n04133789', 'n04141076', 'n04146614', 'n04147183', 'n04192698', 'n04254680', 'n04266014', 'n04275548', 'n04310018', 'n04325704', 'n04347754', 'n04389033', 'n04409515', 'n04465501', 'n04487394', 'n04522168', 'n04536866', 'n04552348', 'n04591713', 'n07614500', 'n07693725', 'n07695742', 'n07697313', 'n07697537', 'n07714571', 'n07714990', 'n07718472', 'n07720875', 'n07734744', 'n07742313', 'n07745940', 'n07749582', 'n07753275', 'n07753592', 'n07768694', 'n07873807', 'n07880968', 'n07920052', 'n09472597', 'n09835506', 'n10565667', 'n12267677']\n",
    "imagenet_r_wnids.sort()\n",
    "classes_chosen = imagenet_r_wnids[::2] # Choose 100 classes for our dataset\n",
    "assert len(classes_chosen) == 100\n",
    "\n",
    "def make_block(hidden_planes=64):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, hidden_planes, kernel_size=5, stride=1, padding=2),\n",
    "        # nn.BatchNorm2d(hidden_planes),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(hidden_planes, hidden_planes, kernel_size=5, stride=1, padding=2, groups=8),\n",
    "        # nn.BatchNorm2d(hidden_planes),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(hidden_planes, hidden_planes, kernel_size=5, stride=1, padding=2, groups=8),\n",
    "        # nn.BatchNorm2d(hidden_planes),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(hidden_planes, 3, kernel_size=3, stride=1, padding=1)\n",
    "    )\n",
    "\n",
    "class BlockWrapper(torch.nn.Module):\n",
    "    def __init__(self, hidden_planes=64, advnet_norm_factor=0.2):\n",
    "        super(BlockWrapper, self).__init__()\n",
    "\n",
    "        self.block1 = make_block(hidden_planes=hidden_planes)\n",
    "        self.block2 = make_block(hidden_planes=hidden_planes)\n",
    "        self.advnet_norm_factor = advnet_norm_factor\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        select = torch.tensor([random.random() < 1.0 for _ in range(batch_size)])\n",
    "        x_advnet = x[select]\n",
    "\n",
    "        block_out = self.block1(x_advnet)\n",
    "        factor = (torch.norm(x_advnet) / torch.norm(block_out))\n",
    "        x_advnet = block_out * factor * self.advnet_norm_factor + x_advnet\n",
    "\n",
    "        block_out = self.block2(x_advnet)\n",
    "        factor = (torch.norm(x_advnet) / torch.norm(block_out))\n",
    "        x_advnet = block_out * factor * self.advnet_norm_factor + x_advnet\n",
    "\n",
    "        x[select] = x_advnet\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelResNet(torch.nn.Module):\n",
    "    def __init__(self, epsilon=0.2, hidden_planes=64, advnet_norm_factor=0.2, num_parallel=len(classes_chosen)):\n",
    "        super(ParallelResNet, self).__init__()\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.hidden_planes = hidden_planes\n",
    "        self.advnet_norm_factor = advnet_norm_factor\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.DataParallel(\n",
    "                    BlockWrapper(hidden_planes=hidden_planes, advnet_norm_factor=advnet_norm_factor)\n",
    "                ) \n",
    "                for i in range(num_parallel)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, road):\n",
    "        \n",
    "        # Assume we put this on GPU before and remove it after.\n",
    "        block_chosen = self.blocks[road]\n",
    "        x = block_chosen(x)\n",
    "\n",
    "        return {\"output\" : x}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advnet = ParallelResNet(\n",
    "    epsilon=0.5, \n",
    "    advnet_norm_factor=0.1\n",
    ")\n",
    "\n",
    "MODEL_PATH = \"/accounts/projects/jsteinhardt/sauravkadavath/imagenet-classifier/checkpoints/advnetOnly_train/model.pth.tar\"\n",
    "advnet.load_state_dict(torch.load(MODEL_PATH)['advnet_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ImageNetSubsetDataset(datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class to take a specified subset of some larger dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, root, *args, **kwargs):\n",
    "        \n",
    "        print(\"Using {0} classes {1}\".format(len(classes_chosen), classes_chosen))\n",
    "\n",
    "        self.new_root = tempfile.mkdtemp()\n",
    "        for _class in classes_chosen:\n",
    "            orig_dir = os.path.join(root, _class)\n",
    "            assert os.path.isdir(orig_dir)\n",
    "\n",
    "            os.symlink(orig_dir, os.path.join(self.new_root, _class))\n",
    "        \n",
    "        super().__init__(self.new_root, *args, **kwargs)\n",
    "    \n",
    "    def __del__(self):\n",
    "        # Clean up\n",
    "        shutil.rmtree(self.new_root)\n",
    "\n",
    "        \n",
    "val_dataset = ImageNetSubsetDataset(\n",
    "    \"/var/tmp/namespace/hendrycks/imagenet/val\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    num_workers=args.workers, \n",
    "    pin_memory=True, \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
